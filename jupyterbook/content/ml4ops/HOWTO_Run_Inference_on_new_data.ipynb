{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "primary-consensus",
   "metadata": {},
   "source": [
    "# Run inference\n",
    "\n",
    "> Run inference on Sentinel-2 images using a trained model\n",
    "\n",
    "This notebook shows how to load a trained model from a experiment config file. With that model we will then make predictions on new Sentinel-2 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "straight-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "from pyprojroot import here\n",
    "# spyder up to find the root\n",
    "root = here(project_files=[\".here\"])\n",
    "# append to path\n",
    "sys.path.append(str(root))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-block",
   "metadata": {},
   "source": [
    "## Step 1: Get config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ordinary-crazy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Config for experiment:  WFV1_unet\n",
      "{   'data_params': {   'batch_size': 32,\n",
      "                       'bucket_id': 'ml4floods',\n",
      "                       'channel_configuration': 'all',\n",
      "                       'filter_windows': False,\n",
      "                       'input_folder': 'S2',\n",
      "                       'loader_type': 'local',\n",
      "                       'num_workers': 8,\n",
      "                       'path_to_splits': '/worldfloods/public',\n",
      "                       'target_folder': 'gt',\n",
      "                       'test_transformation': {   'normalize': True,\n",
      "                                                  'num_classes': 3,\n",
      "                                                  'totensor': True},\n",
      "                       'train_test_split_file': 'worldfloods/public/train_test_split.json',\n",
      "                       'train_transformation': {   'normalize': True,\n",
      "                                                   'num_classes': 3,\n",
      "                                                   'totensor': True},\n",
      "                       'window_size': [256, 256]},\n",
      "    'deploy': False,\n",
      "    'experiment_name': 'WFV1_unet',\n",
      "    'gpus': '0',\n",
      "    'model_params': {   'hyperparameters': {   'channel_configuration': 'all',\n",
      "                                               'label_names': [   'land',\n",
      "                                                                  'water',\n",
      "                                                                  'cloud'],\n",
      "                                               'lr': 0.0001,\n",
      "                                               'lr_decay': 0.5,\n",
      "                                               'lr_patience': 2,\n",
      "                                               'max_epochs': 25,\n",
      "                                               'max_tile_size': 256,\n",
      "                                               'model_type': 'unet',\n",
      "                                               'num_channels': 13,\n",
      "                                               'num_classes': 3,\n",
      "                                               'val_every': 1,\n",
      "                                               'weight_per_class': [   1.93445299,\n",
      "                                                                       36.60054169,\n",
      "                                                                       2.19400729]},\n",
      "                        'model_folder': 'gs://ml4cc_data_lake/0_DEV/2_Mart/2_MLModelMart',\n",
      "                        'path_to_weights': 'checkpoints/',\n",
      "                        'test': False,\n",
      "                        'train': True,\n",
      "                        'use_pretrained_weights': False},\n",
      "    'resume_from_checkpoint': False,\n",
      "    'seed': 12,\n",
      "    'test': False,\n",
      "    'train': False,\n",
      "    'wandb_entity': 'ml4floods',\n",
      "    'wandb_project': 'worldfloods'}\n"
     ]
    }
   ],
   "source": [
    "from src.models.config_setup import get_default_config\n",
    "experiment_name = \"WFV1_unet\"\n",
    "checkpoint_name = \"epoch=24-step=153649.ckpt\"\n",
    "\n",
    "# experiment_name = \"WFV1_scnn20\"\n",
    "# checkpoint_name = \"epoch=5-step=24581.ckpt\"\n",
    "\n",
    "config_fp = f\"gs://ml4cc_data_lake/0_DEV/2_Mart/2_MLModelMart/{experiment_name}/config.json\"\n",
    "# config_fp = os.path.join(root, 'src', 'models', 'configurations', 'worldfloods_template.json')\n",
    "config = get_default_config(config_fp)\n",
    "\n",
    "# The max_tile_size param controls the max size of patches that are fed to the NN. If you're in a memory contrained environment set this value to 128\n",
    "config[\"model_params\"][\"hyperparameters\"][\"max_tile_size\"] = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-reverse",
   "metadata": {},
   "source": [
    "## Step 2: Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "homeless-train",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorldFloodsModel(\n",
       "  (network): UNet(\n",
       "    (dconv_down1): Sequential(\n",
       "      (0): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (dconv_down2): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (dconv_down3): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (dconv_down4): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dconv_up3): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (dconv_up2): Sequential(\n",
       "      (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (dconv_up1): Sequential(\n",
       "      (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_last): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.worldfloods_model import WorldFloodsModel\n",
    "\n",
    "checkpoint_path = f\"gs://ml4cc_data_lake/0_DEV/2_Mart/2_MLModelMart/{experiment_name}/checkpoint/{checkpoint_name}\"\n",
    "model = WorldFloodsModel.load_from_checkpoint(checkpoint_path)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intense-group",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting model inference function\n",
      "Max tile size: 1024\n"
     ]
    }
   ],
   "source": [
    "from src.models.model_setup import get_model_inference_function\n",
    "\n",
    "inference_function = get_model_inference_function(model, config,apply_normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-point",
   "metadata": {},
   "source": [
    "## Step 3: Helper functions for plotting and reading some demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "positive-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "from rasterio import plot as rasterioplt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from src.data.worldfloods.configs import BANDS_S2\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def read_inference_pair(tiff_inputs:str, folder_ground_truth:str, \n",
    "                        window:Optional[Union[rasterio.windows.Window, Tuple[slice,slice]]], \n",
    "                        return_ground_truth: bool=False, channels:bool=None, \n",
    "                        folder_permanent_water=Optional[str]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, rasterio.Affine]:\n",
    "    \"\"\"\n",
    "    Read a pair of layers from the worldfloods bucket and return them as Tensors to pass to a model, return the transform for plotting with lat/long\n",
    "    \n",
    "    Args:\n",
    "        tiff_inputs: filename for layer in worldfloods bucket\n",
    "        folder_ground_truth: folder name to be replaced by S2 in the input\n",
    "        window: window of layer to use\n",
    "        return_ground_truth: flag to indicate if paired gt layer should be returned\n",
    "        channels: list of channels to read from the image\n",
    "        return_permanent_water: Read permanent water layer raster\n",
    "    \n",
    "    Returns:\n",
    "        (torch_inputs, torch_targets, transform): inputs Tensor, gt Tensor, transform for plotting with lat/long\n",
    "    \"\"\"\n",
    "    \n",
    "    tiff_targets = tiff_inputs.replace(\"/S2/\", folder_ground_truth)\n",
    "\n",
    "    with rasterio.open(tiff_inputs, \"r\") as rst:\n",
    "        inputs = rst.read((np.array(channels) + 1).tolist(), window=window)\n",
    "        # Shifted transform based on the given window (used for plotting)\n",
    "        transform = rst.transform if window is None else rasterio.windows.transform(window, rst.transform)\n",
    "        torch_inputs = torch.Tensor(inputs.astype(np.float32)).unsqueeze(0)\n",
    "    \n",
    "    if folder_permanent_water is not None:\n",
    "        tiff_permanent_water = tiff_inputs.replace(\"/S2/\", folder_permanent_water)\n",
    "        with rasterio.open(tiff_permanent_water, \"r\") as rst:\n",
    "            permanent_water = rst.read(1, window=window)  \n",
    "            torch_permanent_water = torch.tensor(permanent_water)\n",
    "    else:\n",
    "        torch_permanent_water = torch.zeros_like(torch_inputs)\n",
    "        \n",
    "    if return_ground_truth:\n",
    "        with rasterio.open(tiff_targets, \"r\") as rst:\n",
    "            targets = rst.read(1, window=window)\n",
    "        \n",
    "        torch_targets = torch.tensor(targets).unsqueeze(0)\n",
    "    else:\n",
    "        torch_targets = torch.zeros_like(torch_inputs)\n",
    "    \n",
    "    return torch_inputs, torch_targets, torch_permanent_water, transform\n",
    "\n",
    "COLORS_WORLDFLOODS = np.array([[0, 0, 0], # invalid\n",
    "                               [139, 64, 0], # land\n",
    "                               [0, 0, 139], # water\n",
    "                               [220, 220, 220]], # cloud\n",
    "                              dtype=np.float32) / 255\n",
    "\n",
    "INTERPRETATION_WORLDFLOODS = [\"invalid\", \"land\", \"water\", \"cloud\"]\n",
    "\n",
    "COLORS_WORLDFLOODS_PERMANENT = np.array([[0, 0, 0], # 0: invalid\n",
    "                                         [139, 64, 0], # 1: land\n",
    "                                         [237, 0, 0], # 2: flood_water\n",
    "                                         [220, 220, 220], # 3: cloud\n",
    "                                         [0, 0, 139], # 4: permanent_water\n",
    "                                         [60, 85, 92]], # 5: seasonal_water\n",
    "                                        dtype=np.float32) / 255\n",
    "\n",
    "INTERPRETATION_WORLDFLOODS_PERMANENT = [\"invalid\", \"land\", \"flood water\", \"cloud\", \"permanent water\", \"seasonal water\"]\n",
    "\n",
    "def gt_with_permanent_water(gt: np.ndarray, permanent_water: np.ndarray)->np.ndarray:\n",
    "    \"\"\" Permanent water taken from: https://developers.google.com/earth-engine/datasets/catalog/JRC_GSW1_2_YearlyHistory\"\"\"\n",
    "    gt[(gt == 2) & (permanent_water == 3)] = 4 # set as permanent_water\n",
    "    gt[(gt == 2) & (permanent_water == 2)] = 5 # set as seasonal water\n",
    "        \n",
    "    return gt\n",
    "            \n",
    "\n",
    "def get_cmap_norm_colors(color_array, interpretation_array):\n",
    "    cmap_categorical = colors.ListedColormap(color_array)\n",
    "    norm_categorical = colors.Normalize(vmin=-.5,\n",
    "                                        vmax=color_array.shape[0]-.5)\n",
    "    patches = []\n",
    "    for c, interp in zip(color_array, interpretation_array):\n",
    "        patches.append(mpatches.Patch(color=c, label=interp))\n",
    "    \n",
    "    return cmap_categorical, norm_categorical, patches\n",
    "\n",
    "\n",
    "def plot_inference_set(inputs: torch.Tensor, targets: torch.Tensor, \n",
    "                       predictions: torch.Tensor, permanent_water: torch.Tensor, transform: rasterio.Affine)->None:\n",
    "    \"\"\"\n",
    "    Plots inputs, targets and prediction into lat/long visualisation\n",
    "    \n",
    "    Args:\n",
    "        inputs: input Tensor\n",
    "        targets: gt target Tensor\n",
    "        prediction: predictions output by model (softmax, argmax already applied)\n",
    "        permanent_water: permanent water raster\n",
    "        transform: transform used to plot with lat/long\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(2,2,figsize=(16,16))\n",
    "    \n",
    "    inputs_show = inputs.cpu().numpy().squeeze()\n",
    "    targets_show = targets.cpu().numpy().squeeze()\n",
    "    permanent_water_show = permanent_water.numpy().squeeze()\n",
    "    \n",
    "    targets_show = gt_with_permanent_water(targets_show, permanent_water_show)\n",
    "    \n",
    "    \n",
    "    # Color categories {-1: invalid, 0: land, 1: water, 2: clouds}\n",
    "    \n",
    "    cmap_preds, norm_preds, patches_preds = get_cmap_norm_colors(COLORS_WORLDFLOODS, INTERPRETATION_WORLDFLOODS)\n",
    "    cmap_gt, norm_gt, patches_gt = get_cmap_norm_colors(COLORS_WORLDFLOODS_PERMANENT, INTERPRETATION_WORLDFLOODS_PERMANENT)\n",
    "    \n",
    "    # +1 because value 0 is invalid\n",
    "    prediction_show = (predictions + 1).cpu().numpy().astype(float)\n",
    "\n",
    "    rgb = np.clip(inputs_show[[3,2,1], :, :]/3000.,0,1)\n",
    "    \n",
    "    bands_false_composite = [BANDS_S2.index(b) for b in [\"B11\", \"B8\", \"B4\"]] # swir_1, nir, red composite\n",
    "    false_rgb = np.clip(inputs_show[bands_false_composite, :, :]/3000.,0,1)\n",
    "    \n",
    "\n",
    "    rasterioplt.show(rgb,transform=transform,ax=ax[0,0])\n",
    "    ax[0,0].set_title(\"RGB Composite\")\n",
    "    rasterioplt.show(false_rgb,transform=transform,ax=ax[0,1])\n",
    "    ax[0,1].set_title(\"SWIR1,NIR,R Composite\")\n",
    "    rasterioplt.show(targets_show,transform=transform,ax=ax[1,0], cmap=cmap_gt, norm=norm_gt,\n",
    "                     interpolation='nearest')\n",
    "    rasterioplt.show(prediction_show, transform=transform, ax=ax[1,1],cmap=cmap_preds, norm=norm_preds,\n",
    "                     interpolation='nearest')\n",
    "    \n",
    "    ax[1,0].set_title(\"Ground Truth\")\n",
    "    ax[1,0].legend(handles=patches_gt,\n",
    "                 loc='upper right')\n",
    "    \n",
    "    ax[1,1].set_title(\"Model prediction\")\n",
    "    ax[1,1].legend(handles=patches_preds,\n",
    "                   loc='upper right')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-village",
   "metadata": {},
   "source": [
    "## Perform Inference using the `inference_function`\n",
    "\n",
    "Note: in order to read GeoTIFF files from the google bucket directly with `rasterio` you must authenticate in the Google Cloud Platform.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "seeing-examination",
   "metadata": {},
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "GS_SECRET_ACCESS_KEY+GS_ACCESS_KEY_ID, GS_OAUTH2_REFRESH_TOKEN or GOOGLE_APPLICATION_CREDENTIALS or GS_OAUTH2_PRIVATE_KEY+GS_OAUTH2_CLIENT_EMAIL configuration options and /home/gonzalo/.boto not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_AWSInvalidCredentialsError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_shim.pyx\u001b[0m in \u001b[0;36mrasterio._shim.open_dataset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_err.pyx\u001b[0m in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_AWSInvalidCredentialsError\u001b[0m: GS_SECRET_ACCESS_KEY+GS_ACCESS_KEY_ID, GS_OAUTH2_REFRESH_TOKEN or GOOGLE_APPLICATION_CREDENTIALS or GS_OAUTH2_PRIVATE_KEY+GS_OAUTH2_CLIENT_EMAIL configuration options and /home/gonzalo/.boto not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ae1684726968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the image and ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m torch_inputs, torch_targets, torch_permanent_water, transform = read_inference_pair(tiff_s2,folder_ground_truth=\"/gt/\", \n\u001b[0m\u001b[1;32m      7\u001b[0m                                                                                     \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ground_truth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                                                     folder_permanent_water=\"/PERMANENTWATERJRC/\")\n",
      "\u001b[0;32m~/miniconda3/envs/ml4fl_py38/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-927bdc7f3ec1>\u001b[0m in \u001b[0;36mread_inference_pair\u001b[0;34m(tiff_inputs, folder_ground_truth, window, return_ground_truth, channels, folder_permanent_water)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtiff_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiff_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/S2/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_ground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiff_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Shifted transform based on the given window (used for plotting)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4fl_py38/lib/python3.8/site-packages/rasterio/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml4fl_py38/lib/python3.8/site-packages/rasterio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             s = get_writer_for_path(path, driver=driver)(\n",
      "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: GS_SECRET_ACCESS_KEY+GS_ACCESS_KEY_ID, GS_OAUTH2_REFRESH_TOKEN or GOOGLE_APPLICATION_CREDENTIALS or GS_OAUTH2_PRIVATE_KEY+GS_OAUTH2_CLIENT_EMAIL configuration options and /home/gonzalo/.boto not defined"
     ]
    }
   ],
   "source": [
    "from src.models.model_setup import get_channel_configuration_bands\n",
    "\n",
    "tiff_s2, window, channels = \"gs://ml4floods/worldfloods/public/test/S2/EMSR333_02PORTOPALO_DEL_MONIT01_v1_observed_event_a.tif\", (slice(1000,None),slice(0,400)), get_channel_configuration_bands(config.model_params.hyperparameters.channel_configuration)\n",
    "\n",
    "# Load the image and ground truth\n",
    "torch_inputs, torch_targets, torch_permanent_water, transform = read_inference_pair(tiff_s2,folder_ground_truth=\"/gt/\", \n",
    "                                                                                    window=window, return_ground_truth=True, channels=channels,\n",
    "                                                                                    folder_permanent_water=\"/PERMANENTWATERJRC/\")\n",
    "\n",
    "# Compute the prediction\n",
    "outputs = inference_function(torch_inputs) # (batch_size, num_classes, h, w)\n",
    "prediction = torch.argmax(outputs, dim=1).long() # (batch_size, h, w)\n",
    "plot_inference_set(torch_inputs, torch_targets, prediction, torch_permanent_water, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-eight",
   "metadata": {},
   "source": [
    "## Lets try another image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_s2, window, channels = \"gs://ml4floods/worldfloods/public/test/S2/EMSR347_07ZOMBA_DEL_v2_observed_event_a.tif\", None, get_channel_configuration_bands(config.model_params.hyperparameters.channel_configuration)\n",
    "\n",
    "torch_inputs, torch_targets, torch_permanent_water, transform = read_inference_pair(tiff_s2, folder_ground_truth=\"/gt/\", \n",
    "                                                                                    window=window, \n",
    "                                                                                    return_ground_truth=True, channels=channels,\n",
    "                                                                                    folder_permanent_water=\"/PERMANENTWATERJRC/\")\n",
    "\n",
    "outputs = inference_function(torch_inputs) # (batch_size, num_classes, h, w)\n",
    "prediction = torch.argmax(outputs, dim=1).long() # (batch_size, h, w)\n",
    "plot_inference_set(torch_inputs, torch_targets, prediction, torch_permanent_water, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-delhi",
   "metadata": {},
   "source": [
    "## Lets try another image from the new data prepared by the Janitors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio.windows \n",
    "window = rasterio.windows.Window(col_off=1543, row_off=247, \n",
    "                                 width=2000, height=2000)\n",
    "tiff_s2, channels = \"gs://ml4cc_data_lake/0_DEV/1_Staging/WorldFloods/S2/EMSR501/AOI01/EMSR501_AOI01_DEL_MONIT01_r1_v1.tif\", get_channel_configuration_bands(config.model_params.hyperparameters.channel_configuration)\n",
    "\n",
    "torch_inputs, torch_targets, torch_permanent_water, transform = read_inference_pair(tiff_s2, folder_ground_truth=\"/GT/V_1_1/\", \n",
    "                                                                                    window=window, \n",
    "                                                                                    return_ground_truth=True, channels=channels,\n",
    "                                                                                    folder_permanent_water=\"/JRC/\")\n",
    "\n",
    "outputs = inference_function(torch_inputs) # (batch_size, num_classes, h, w)\n",
    "prediction = torch.argmax(outputs, dim=1).long() # (batch_size, h, w)\n",
    "plot_inference_set(torch_inputs, torch_targets, prediction, torch_permanent_water, transform)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4fl_py38]",
   "language": "python",
   "name": "conda-env-ml4fl_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
