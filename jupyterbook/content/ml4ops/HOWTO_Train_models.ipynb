{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "closed-magnitude",
   "metadata": {},
   "source": [
    "# Train models\n",
    "\n",
    "> Tutorial 1: Train a Flood Extent segmentation model using the WorldFloods dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-multiple",
   "metadata": {},
   "source": [
    "## Step 0: Notebook setup\n",
    "    - Configure notebook basics\n",
    "    - Configure GCP Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "auburn-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-supply",
   "metadata": {},
   "source": [
    "## Step 1: Setup Configuration file\n",
    "    - Load configuration file from local device or gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informal-stability",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ml4floods.models.config_setup import get_default_config\n",
    "import pkg_resources\n",
    "\n",
    "# Set filepath to configuration files\n",
    "# config_fp = 'path/to/worldfloods_template.json'\n",
    "config_fp = pkg_resources.resource_filename(\"ml4floods\",\"models/configurations/worldfloods_template.json\")\n",
    "\n",
    "config = get_default_config(config_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-springer",
   "metadata": {},
   "source": [
    "## Step 1.a: Seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acting-dover",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "# Seed\n",
    "seed_everything(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-circuit",
   "metadata": {},
   "source": [
    "## Step 1.b: Make it a unique experiment\n",
    "    - 'experiment_name' is used to specify the folder in which to save models and associated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "royal-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.experiment_name = 'training_demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-craft",
   "metadata": {},
   "source": [
    "## Step 2: Setup Dataloader\n",
    "    - 'loader_type' can be one of 'local' which assumes the images are already saved locally, or 'bucket' which will load images directly from the bucket specified in 'bucket_id'. If set to local and the dataset is not found in the path `config.data_params.path_to_splits` it will trigger the download of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "heavy-efficiency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local dataset for this run\n",
      "train 196648  tiles\n",
      "val 1284  tiles\n",
      "test 11  tiles\n"
     ]
    }
   ],
   "source": [
    "from ml4floods.models.dataset_setup import get_dataset\n",
    "\n",
    "config.data_params.loader_type = 'local'\n",
    "config.data_params.path_to_splits = \"/worldfloods/public/\"\n",
    "dataset = get_dataset(config.data_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-answer",
   "metadata": {},
   "source": [
    "## Step 3: Setup Model\n",
    "     - 'train' = True specifies that we are training a new model from scratch\n",
    "     - get_model(args) constructs a pytorch lightning model using the configuration specified in 'config.model_params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closing-singles",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_folder': 'models',\n",
       " 'model_version': 'v1',\n",
       " 'hyperparameters': {'max_tile_size': 256,\n",
       "  'metric_monitor': 'val_dice_loss',\n",
       "  'channel_configuration': 'all',\n",
       "  'label_names': ['land', 'water', 'cloud'],\n",
       "  'weight_per_class': [1.93445299, 36.60054169, 2.19400729],\n",
       "  'model_type': 'linear',\n",
       "  'num_classes': 3,\n",
       "  'max_epochs': 10,\n",
       "  'val_every': 1,\n",
       "  'lr': 0.0001,\n",
       "  'lr_decay': 0.5,\n",
       "  'lr_patience': 2,\n",
       "  'num_channels': 13},\n",
       " 'train': True,\n",
       " 'test': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # folder to store the trained model (it will create a subfolder with the name of the experiment)\n",
    "config.model_params.model_folder = \"models\" \n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "config.model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fatal-wheel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WorldFloodsModel(\n",
       "  (network): SimpleLinear(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(13, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml4floods.models.model_setup import get_model\n",
    "\n",
    "config.model_params.test = False\n",
    "config.model_params.train = True\n",
    "model = get_model(config.model_params)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-latex",
   "metadata": {},
   "source": [
    "## Step 4: Set up Weights and Biases Logger for experiment\n",
    "    - We pass this to the model trainer in a later cell to automaticall log relevant metrics to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "identical-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# UNCOMMENT ON FIRST RUN TO LOGIN TO Weights and Biases (only needs to be done once)\n",
    "# wandb.login()\n",
    "# run = wandb.init()\n",
    "\n",
    "# Specifies who is logging the experiment to wandb\n",
    "config['wandb_entity'] = 'ml4floods'\n",
    "# Specifies which wandb project to log to, multiple runs can exist in the same project\n",
    "config['wandb_project'] = 'worldfloods-notebook-demo-project'\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    name=config.experiment_name,\n",
    "    project=config.wandb_project, \n",
    "    entity=config.wandb_entity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-adolescent",
   "metadata": {},
   "source": [
    "### Step 5: Setup Lightning Callbacks\n",
    "    - We implement checkpointing using the ModelCheckpoint callback to save the best performing checkpoints to local/gcs storage\n",
    "    - We implement early stopping using the EarlyStopping callback to stop training early if there is no performance improvement after 10 epochs from the latest best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "black-think",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/training_demo\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "experiment_path = f\"{config.model_params.model_folder}/{config.experiment_name}\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"{experiment_path}/checkpoint\",\n",
    "    save_top_k=True,\n",
    "    verbose=True,\n",
    "    monitor='val_dice_loss',\n",
    "    mode='min',\n",
    "    prefix=''\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_dice_loss',\n",
    "    patience=10,\n",
    "    strict=False,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint_callback, early_stop_callback]\n",
    "\n",
    "print(f\"{config.model_params.model_folder}/{config.experiment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-transaction",
   "metadata": {},
   "source": [
    "## Step 6: Setup Lighting Trainer\n",
    "    -- Pytorch Lightning Trainer handles all the rest of the model training for us!\n",
    "    -- add flags from \n",
    "    https://pytorch-lightning.readthedocs.io/en/0.7.5/trainer.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "organized-venice",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "config.gpus = '0'  # which gpu to use\n",
    "\n",
    "config.model_params.hyperparameters.max_epochs = 4 # train for maximum 4 epochs\n",
    "\n",
    "trainer = Trainer(\n",
    "    fast_dev_run=False,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=callbacks,\n",
    "    default_root_dir=f\"{config.model_params.model_folder}/{config.experiment_name}\",\n",
    "    accumulate_grad_batches=1,\n",
    "    gradient_clip_val=0.0,\n",
    "    auto_lr_find=False,\n",
    "    benchmark=False,\n",
    "    distributed_backend=None,\n",
    "    gpus=config.gpus,\n",
    "    max_epochs=config.model_params.hyperparameters.max_epochs,\n",
    "    check_val_every_n_epoch=config.model_params.hyperparameters.val_every,\n",
    "    log_gpu_memory=None,\n",
    "    resume_from_checkpoint=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-australian",
   "metadata": {},
   "source": [
    "### Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "scenic-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type         | Params\n",
      "-----------------------------------------\n",
      "0 | network | SimpleLinear | 42    \n",
      "-----------------------------------------\n",
      "42        Trainable params\n",
      "0         Non-trainable params\n",
      "42        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ml4floods/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 6147/6187 [46:53<00:18,  2.18it/s, loss=1.16, v_num=wene]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 6149/6187 [46:57<00:17,  2.18it/s, loss=1.16, v_num=wene]\n",
      "Epoch 0:  99%|█████████▉| 6151/6187 [46:59<00:16,  2.18it/s, loss=1.16, v_num=wene]\n",
      "Validating:  12%|█▏        | 5/41 [00:06<00:43,  1.20s/it]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 6153/6187 [47:01<00:15,  2.18it/s, loss=1.16, v_num=wene]\n",
      "Validating:  17%|█▋        | 7/41 [00:09<00:42,  1.26s/it]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 6155/6187 [47:04<00:14,  2.18it/s, loss=1.16, v_num=wene]\n",
      "Validating:  22%|██▏       | 9/41 [00:11<00:40,  1.28s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6157/6187 [47:06<00:13,  2.18it/s, loss=1.16, v_num=wene]\n",
      "Validating:  27%|██▋       | 11/41 [00:14<00:38,  1.29s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6159/6187 [47:09<00:12,  2.18it/s, loss=1.16, v_num=wene]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gonzalo/ml4floods/src/models/utils/metrics.py:139: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  recall = true_positive / (true_positive + false_negative)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating:  32%|███▏      | 13/41 [00:17<00:36,  1.31s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6161/6187 [47:12<00:11,  2.18it/s, loss=1.16, v_num=wene]\n",
      "Validating:  37%|███▋      | 15/41 [00:19<00:33,  1.29s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6163/6187 [47:14<00:11,  2.17it/s, loss=1.16, v_num=wene]\n",
      "Validating:  41%|████▏     | 17/41 [00:22<00:30,  1.29s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6165/6187 [47:17<00:10,  2.17it/s, loss=1.16, v_num=wene]\n",
      "Validating:  46%|████▋     | 19/41 [00:24<00:28,  1.29s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6167/6187 [47:19<00:09,  2.17it/s, loss=1.16, v_num=wene]\n",
      "Validating:  51%|█████     | 21/41 [00:27<00:25,  1.28s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6169/6187 [47:22<00:08,  2.17it/s, loss=1.16, v_num=wene]\n",
      "Validating:  56%|█████▌    | 23/41 [00:29<00:22,  1.25s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6171/6187 [47:24<00:07,  2.17it/s, loss=1.16, v_num=wene]\n",
      "Validating:  61%|██████    | 25/41 [00:32<00:20,  1.29s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6173/6187 [47:27<00:06,  2.17it/s, loss=1.16, v_num=wene]\n",
      "Validating:  66%|██████▌   | 27/41 [00:35<00:18,  1.31s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6175/6187 [47:30<00:05,  2.17it/s, loss=1.16, v_num=wene]\n",
      "Validating:  71%|███████   | 29/41 [00:37<00:15,  1.31s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6177/6187 [47:32<00:04,  2.17it/s, loss=1.16, v_num=wene]\n",
      "Validating:  76%|███████▌  | 31/41 [00:40<00:13,  1.31s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6179/6187 [47:35<00:03,  2.16it/s, loss=1.16, v_num=wene]\n",
      "Validating:  80%|████████  | 33/41 [00:42<00:10,  1.30s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6181/6187 [47:38<00:02,  2.16it/s, loss=1.16, v_num=wene]\n",
      "Validating:  85%|████████▌ | 35/41 [00:45<00:07,  1.30s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6183/6187 [47:40<00:01,  2.16it/s, loss=1.16, v_num=wene]\n",
      "Validating:  90%|█████████ | 37/41 [00:47<00:04,  1.24s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 6185/6187 [47:43<00:00,  2.16it/s, loss=1.16, v_num=wene]\n",
      "Validating:  95%|█████████▌| 39/41 [00:50<00:02,  1.28s/it]\u001b[A\n",
      "Epoch 0: 100%|██████████| 6187/6187 [47:45<00:00,  2.16it/s, loss=1.16, v_num=wene]\n",
      "                                                           \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 6145: val_dice_loss reached 0.71664 (best 0.71664), saving model to \"gs://ml4cc_data_lake/0_DEV/2_Mart/2_MLModelMart/worldfloods-notebook-training-demo/checkpoint/epoch=0-step=6145.ckpt\" as top True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|█████████▉| 6146/6187 [46:36<00:18,  2.20it/s, loss=1.05, v_num=wene] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 6148/6187 [46:40<00:17,  2.20it/s, loss=1.05, v_num=wene]\n",
      "Epoch 1:  99%|█████████▉| 6150/6187 [46:40<00:16,  2.20it/s, loss=1.05, v_num=wene]\n",
      "Validating:  10%|▉         | 4/41 [00:05<00:42,  1.16s/it]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 6152/6187 [46:43<00:15,  2.19it/s, loss=1.05, v_num=wene]\n",
      "Validating:  15%|█▍        | 6/41 [00:07<00:43,  1.25s/it]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 6154/6187 [46:46<00:15,  2.19it/s, loss=1.05, v_num=wene]\n",
      "Validating:  20%|█▉        | 8/41 [00:10<00:42,  1.28s/it]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 6156/6187 [46:48<00:14,  2.19it/s, loss=1.05, v_num=wene]\n",
      "Validating:  24%|██▍       | 10/41 [00:13<00:40,  1.31s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6158/6187 [46:51<00:13,  2.19it/s, loss=1.05, v_num=wene]\n",
      "Validating:  29%|██▉       | 12/41 [00:15<00:38,  1.33s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6160/6187 [46:54<00:12,  2.19it/s, loss=1.05, v_num=wene]\n",
      "Validating:  34%|███▍      | 14/41 [00:18<00:35,  1.32s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6162/6187 [46:56<00:11,  2.19it/s, loss=1.05, v_num=wene]\n",
      "Validating:  39%|███▉      | 16/41 [00:21<00:32,  1.30s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6164/6187 [46:59<00:10,  2.19it/s, loss=1.05, v_num=wene]\n",
      "Validating:  44%|████▍     | 18/41 [00:23<00:29,  1.30s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6166/6187 [47:01<00:09,  2.19it/s, loss=1.05, v_num=wene]\n",
      "Validating:  49%|████▉     | 20/41 [00:26<00:27,  1.29s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6168/6187 [47:04<00:08,  2.18it/s, loss=1.05, v_num=wene]\n",
      "Validating:  54%|█████▎    | 22/41 [00:28<00:23,  1.22s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6170/6187 [47:06<00:07,  2.18it/s, loss=1.05, v_num=wene]\n",
      "Validating:  59%|█████▊    | 24/41 [00:31<00:21,  1.28s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6172/6187 [47:09<00:06,  2.18it/s, loss=1.05, v_num=wene]\n",
      "Validating:  63%|██████▎   | 26/41 [00:34<00:19,  1.31s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6174/6187 [47:12<00:05,  2.18it/s, loss=1.05, v_num=wene]\n",
      "Validating:  68%|██████▊   | 28/41 [00:36<00:17,  1.32s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6176/6187 [47:14<00:05,  2.18it/s, loss=1.05, v_num=wene]\n",
      "Validating:  73%|███████▎  | 30/41 [00:39<00:14,  1.32s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6178/6187 [47:17<00:04,  2.18it/s, loss=1.05, v_num=wene]\n",
      "Validating:  78%|███████▊  | 32/41 [00:41<00:11,  1.31s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6180/6187 [47:19<00:03,  2.18it/s, loss=1.05, v_num=wene]\n",
      "Validating:  83%|████████▎ | 34/41 [00:44<00:09,  1.30s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6182/6187 [47:22<00:02,  2.17it/s, loss=1.05, v_num=wene]\n",
      "Validating:  88%|████████▊ | 36/41 [00:47<00:06,  1.31s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6184/6187 [47:25<00:01,  2.17it/s, loss=1.05, v_num=wene]\n",
      "Validating:  93%|█████████▎| 38/41 [00:49<00:03,  1.27s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 6186/6187 [47:27<00:00,  2.17it/s, loss=1.05, v_num=wene]\n",
      "Epoch 1: 100%|██████████| 6187/6187 [47:29<00:00,  2.17it/s, loss=1.05, v_num=wene]\n",
      "                                                           \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 12291: val_dice_loss reached 0.68838 (best 0.68838), saving model to \"gs://ml4cc_data_lake/0_DEV/2_Mart/2_MLModelMart/worldfloods-notebook-training-demo/checkpoint/epoch=1-step=12291.ckpt\" as top True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  99%|█████████▉| 6146/6187 [47:08<00:18,  2.17it/s, loss=0.957, v_num=wene]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 6148/6187 [47:12<00:17,  2.17it/s, loss=0.957, v_num=wene]\n",
      "Epoch 2:  99%|█████████▉| 6150/6187 [47:12<00:17,  2.17it/s, loss=0.957, v_num=wene]\n",
      "Validating:  10%|▉         | 4/41 [00:05<00:44,  1.20s/it]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 6152/6187 [47:15<00:16,  2.17it/s, loss=0.957, v_num=wene]\n",
      "Validating:  15%|█▍        | 6/41 [00:08<00:45,  1.30s/it]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 6154/6187 [47:18<00:15,  2.17it/s, loss=0.957, v_num=wene]\n",
      "Validating:  20%|█▉        | 8/41 [00:10<00:43,  1.33s/it]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 6156/6187 [47:20<00:14,  2.17it/s, loss=0.957, v_num=wene]\n",
      "Validating:  24%|██▍       | 10/41 [00:13<00:41,  1.35s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6158/6187 [47:23<00:13,  2.17it/s, loss=0.957, v_num=wene]\n",
      "Validating:  29%|██▉       | 12/41 [00:16<00:40,  1.38s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6160/6187 [47:26<00:12,  2.16it/s, loss=0.957, v_num=wene]\n",
      "Validating:  34%|███▍      | 14/41 [00:19<00:37,  1.39s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6162/6187 [47:29<00:11,  2.16it/s, loss=0.957, v_num=wene]\n",
      "Validating:  39%|███▉      | 16/41 [00:22<00:34,  1.37s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6164/6187 [47:32<00:10,  2.16it/s, loss=0.957, v_num=wene]\n",
      "Validating:  44%|████▍     | 18/41 [00:24<00:31,  1.36s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6166/6187 [47:34<00:09,  2.16it/s, loss=0.957, v_num=wene]\n",
      "Validating:  49%|████▉     | 20/41 [00:27<00:28,  1.36s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6168/6187 [47:37<00:08,  2.16it/s, loss=0.957, v_num=wene]\n",
      "Validating:  54%|█████▎    | 22/41 [00:29<00:24,  1.28s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6170/6187 [47:39<00:07,  2.16it/s, loss=0.957, v_num=wene]\n",
      "Validating:  59%|█████▊    | 24/41 [00:32<00:22,  1.33s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6172/6187 [47:42<00:06,  2.16it/s, loss=0.957, v_num=wene]\n",
      "Validating:  63%|██████▎   | 26/41 [00:35<00:20,  1.36s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6174/6187 [47:45<00:06,  2.15it/s, loss=0.957, v_num=wene]\n",
      "Validating:  68%|██████▊   | 28/41 [00:38<00:17,  1.37s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6176/6187 [47:48<00:05,  2.15it/s, loss=0.957, v_num=wene]\n",
      "Validating:  73%|███████▎  | 30/41 [00:40<00:15,  1.39s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6178/6187 [47:51<00:04,  2.15it/s, loss=0.957, v_num=wene]\n",
      "Validating:  78%|███████▊  | 32/41 [00:43<00:12,  1.38s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6180/6187 [47:53<00:03,  2.15it/s, loss=0.957, v_num=wene]\n",
      "Validating:  83%|████████▎ | 34/41 [00:46<00:09,  1.39s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6182/6187 [47:56<00:02,  2.15it/s, loss=0.957, v_num=wene]\n",
      "Validating:  88%|████████▊ | 36/41 [00:49<00:06,  1.38s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6184/6187 [47:59<00:01,  2.15it/s, loss=0.957, v_num=wene]\n",
      "Validating:  93%|█████████▎| 38/41 [00:51<00:04,  1.34s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 6186/6187 [48:01<00:00,  2.15it/s, loss=0.957, v_num=wene]\n",
      "Epoch 2: 100%|██████████| 6187/6187 [48:03<00:00,  2.15it/s, loss=0.957, v_num=wene]\n",
      "                                                           \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 18437: val_dice_loss reached 0.67237 (best 0.67237), saving model to \"gs://ml4cc_data_lake/0_DEV/2_Mart/2_MLModelMart/worldfloods-notebook-training-demo/checkpoint/epoch=2-step=18437.ckpt\" as top True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  99%|█████████▉| 6146/6187 [48:58<00:19,  2.09it/s, loss=0.987, v_num=wene]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 6148/6187 [49:02<00:18,  2.09it/s, loss=0.987, v_num=wene]\n",
      "Epoch 3:  99%|█████████▉| 6150/6187 [49:02<00:17,  2.09it/s, loss=0.987, v_num=wene]\n",
      "Validating:  10%|▉         | 4/41 [00:05<00:43,  1.19s/it]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 6152/6187 [49:05<00:16,  2.09it/s, loss=0.987, v_num=wene]\n",
      "Validating:  15%|█▍        | 6/41 [00:08<00:45,  1.29s/it]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 6154/6187 [49:08<00:15,  2.09it/s, loss=0.987, v_num=wene]\n",
      "Validating:  20%|█▉        | 8/41 [00:10<00:43,  1.33s/it]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 6156/6187 [49:10<00:14,  2.09it/s, loss=0.987, v_num=wene]\n",
      "Validating:  24%|██▍       | 10/41 [00:13<00:41,  1.34s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6158/6187 [49:13<00:13,  2.08it/s, loss=0.987, v_num=wene]\n",
      "Validating:  29%|██▉       | 12/41 [00:16<00:39,  1.36s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6160/6187 [49:16<00:12,  2.08it/s, loss=0.987, v_num=wene]\n",
      "Validating:  34%|███▍      | 14/41 [00:19<00:37,  1.38s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6162/6187 [49:19<00:12,  2.08it/s, loss=0.987, v_num=wene]\n",
      "Validating:  39%|███▉      | 16/41 [00:21<00:33,  1.36s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6164/6187 [49:21<00:11,  2.08it/s, loss=0.987, v_num=wene]\n",
      "Validating:  44%|████▍     | 18/41 [00:24<00:31,  1.37s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6166/6187 [49:24<00:10,  2.08it/s, loss=0.987, v_num=wene]\n",
      "Validating:  49%|████▉     | 20/41 [00:27<00:28,  1.35s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6168/6187 [49:27<00:09,  2.08it/s, loss=0.987, v_num=wene]\n",
      "Validating:  54%|█████▎    | 22/41 [00:29<00:24,  1.29s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6170/6187 [49:29<00:08,  2.08it/s, loss=0.987, v_num=wene]\n",
      "Validating:  59%|█████▊    | 24/41 [00:32<00:22,  1.33s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6172/6187 [49:32<00:07,  2.08it/s, loss=0.987, v_num=wene]\n",
      "Validating:  63%|██████▎   | 26/41 [00:35<00:20,  1.35s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6174/6187 [49:35<00:06,  2.08it/s, loss=0.987, v_num=wene]\n",
      "Validating:  68%|██████▊   | 28/41 [00:38<00:17,  1.38s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6176/6187 [49:38<00:05,  2.07it/s, loss=0.987, v_num=wene]\n",
      "Validating:  73%|███████▎  | 30/41 [00:40<00:15,  1.38s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6178/6187 [49:40<00:04,  2.07it/s, loss=0.987, v_num=wene]\n",
      "Validating:  78%|███████▊  | 32/41 [00:43<00:12,  1.37s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6180/6187 [49:43<00:03,  2.07it/s, loss=0.987, v_num=wene]\n",
      "Validating:  83%|████████▎ | 34/41 [00:46<00:09,  1.37s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6182/6187 [49:46<00:02,  2.07it/s, loss=0.987, v_num=wene]\n",
      "Validating:  88%|████████▊ | 36/41 [00:49<00:06,  1.37s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6184/6187 [49:48<00:01,  2.07it/s, loss=0.987, v_num=wene]\n",
      "Validating:  93%|█████████▎| 38/41 [00:51<00:03,  1.33s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 6186/6187 [49:51<00:00,  2.07it/s, loss=0.987, v_num=wene]\n",
      "Epoch 3: 100%|██████████| 6187/6187 [49:52<00:00,  2.07it/s, loss=0.987, v_num=wene]\n",
      "                                                           \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 24583: val_dice_loss reached 0.66332 (best 0.66332), saving model to \"gs://ml4cc_data_lake/0_DEV/2_Mart/2_MLModelMart/worldfloods-notebook-training-demo/checkpoint/epoch=3-step=24583.ckpt\" as top True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 6187/6187 [49:54<00:00,  2.07it/s, loss=0.987, v_num=wene]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-dealing",
   "metadata": {},
   "source": [
    "## Step 7: Save trained model\n",
    "    - Save model to local/gcs along with configuration file used to conduct training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "material-reserve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "Run pip install nbformat to save notebook history\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 21640<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/gonzalo/ml4floods/notebooks/models/wandb/run-20210318_151018-25j7wene/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/gonzalo/ml4floods/notebooks/models/wandb/run-20210318_151018-25j7wene/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>25660</td></tr><tr><td>_timestamp</td><td>1616105878</td></tr><tr><td>_step</td><td>24583</td></tr><tr><td>loss</td><td>0.87475</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>val_bce_loss</td><td>1.81653</td></tr><tr><td>val_dice_loss</td><td>0.66332</td></tr><tr><td>val_recall land</td><td>0.86352</td></tr><tr><td>val_recall water</td><td>inf</td></tr><tr><td>val_recall cloud</td><td>inf</td></tr><tr><td>val_iou land</td><td>0.78074</td></tr><tr><td>val_iou water</td><td>0.37365</td></tr><tr><td>val_iou cloud</td><td>0.20163</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▆▆▄▄▆▅▄▃▃▅▃▂▂▃▃▃▃▄▅▇▂▂▃▃▁▄▅▂▁▂▂▂▃▆▂▂▂▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▆▆▆▆▆▆▆▆▆▆██████████</td></tr><tr><td>val_bce_loss</td><td>█▄▂▁</td></tr><tr><td>val_dice_loss</td><td>█▄▂▁</td></tr><tr><td>val_recall land</td><td>▁▆██</td></tr><tr><td>val_recall water</td><td></td></tr><tr><td>val_recall cloud</td><td></td></tr><tr><td>val_iou land</td><td>▁▆██</td></tr><tr><td>val_iou water</td><td>▁▅▇█</td></tr><tr><td>val_iou cloud</td><td>▁▅▆█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1728 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">solar-morning-1</strong>: <a href=\"https://wandb.ai/ipl_uv/ml4floods-notebooks_models/runs/25j7wene\" target=\"_blank\">https://wandb.ai/ipl_uv/ml4floods-notebooks_models/runs/25j7wene</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_lightning.utilities.cloud_io import atomic_save\n",
    "from ml4floods.models.config_setup import save_json\n",
    "\n",
    "# Save in the cloud and in the wandb logger save dir\n",
    "atomic_save(model.state_dict(), f\"{experiment_path}/model.pt\")\n",
    "torch.save(model.state_dict(), os.path.join(wandb_logger.save_dir, 'model.pt'))\n",
    "wandb.save(os.path.join(wandb_logger.save_dir, 'model.pt')) # Copy weights to weights and biases server\n",
    "wandb.finish()\n",
    "\n",
    "# Save cofig file in experiment_path\n",
    "config_file_path = f\"{experiment_path}/config.json\"\n",
    "save_json(config, config_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-bahamas",
   "metadata": {},
   "source": [
    "All Done - Now head to the Model Inference Tutorial to see how your model performed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml4fl_py38]",
   "language": "python",
   "name": "conda-env-ml4fl_py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
