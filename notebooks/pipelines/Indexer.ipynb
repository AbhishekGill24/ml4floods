{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radio-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = [\"/tank/ml4cc/nicholas/ml4floods\"] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smooth-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import dateutil\n",
    "import math\n",
    "\n",
    "from src.data.index.geographic_index import GeographicIndex\n",
    "\n",
    "FS_PREFIX = \"/tank/ml4cc/nicholas/worldfloods\"\n",
    "MASTER_PREFIX = \"worldfloods/tiffimages\"\n",
    "METADATA_PREFIX = \"meta\"\n",
    "PROVIDER_GEOTIFF_PREFIXES = [\"S2\", \"L8\", \"gt\"]\n",
    "CLOUDMASK_PREFIX = \"cloudprob\"\n",
    "FLOODMAP_PREFIX = \"floodmaps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caring-reality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lat: [37, 38)\n",
      "Lon: [12, 14)\n",
      "Bounds: [12.857676493131123, 37.57113844301487, 13.01631897230663, 37.69609409903589]\n",
      "705 valid records indexed out of 755\n"
     ]
    }
   ],
   "source": [
    "idx = GeographicIndex()\n",
    "count = 0\n",
    "totalcount = 0\n",
    "for file in os.listdir(os.path.join(FS_PREFIX, MASTER_PREFIX, METADATA_PREFIX)):\n",
    "    if file.endswith(\".json\"):\n",
    "        totalcount += 1\n",
    "        with open(os.path.join(FS_PREFIX, MASTER_PREFIX, METADATA_PREFIX, file), \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "            \n",
    "        # Some metadata is corrupted -- full of NaN. Let's not even try to deal with those records for now.\n",
    "        # These records also tend not to have bounds\n",
    "        if \"bounds\" in metadata:\n",
    "            bounds = metadata[\"bounds\"]\n",
    "            satdate = satdate = dateutil.parser.parse(metadata[\"satellite date\"])\n",
    "            s2satdate = satdate\n",
    "\n",
    "            # Gather up the relevant files and some basic time/date metadata from them\n",
    "            relevant_files = []\n",
    "            basename = os.path.splitext(file)[0]\n",
    "\n",
    "            if \"s2metadata\" in metadata and len(metadata[\"s2metadata\"]) == 1:\n",
    "                s2satdate = satdate = dateutil.parser.parse(metadata[\"s2metadata\"][0][\"date_string\"])\n",
    "\n",
    "            if os.path.exists(os.path.join(FS_PREFIX, MASTER_PREFIX, FLOODMAP_PREFIX, basename+\".shp\")):\n",
    "                relevant_files.append({\"type\": \"floodmap\", \"last_modified\": satdate, \"path\": os.path.join(MASTER_PREFIX, FLOODMAP_PREFIX, basename+\".shp\")})\n",
    "                            \n",
    "            if os.path.exists(os.path.join(FS_PREFIX, MASTER_PREFIX, CLOUDMASK_PREFIX, basename+\".tif\")):\n",
    "                relevant_files.append({\"type\": \"cloudmask\", \"last_modified\": satdate, \"path\": os.path.join(MASTER_PREFIX, CLOUDMASK_PREFIX, basename+\".tif\")})\n",
    "\n",
    "            count += 1\n",
    "            for sat_prefix in PROVIDER_GEOTIFF_PREFIXES:\n",
    "                sat_path = os.path.join(FS_PREFIX, MASTER_PREFIX, sat_prefix, basename+\".tif\")\n",
    "                if os.path.exists(sat_path):\n",
    "                    relevant_files.append({\"type\": \"satellite_image\", \"last_modified\": s2satdate if sat_prefix == \"S2\" else satdate, \"provider_id\": sat_prefix, \"path\": os.path.join(MASTER_PREFIX, sat_prefix, basename+\".tif\")})\n",
    "            \n",
    "            # Add these relevant files to each 1-degree x 1-degree bin\n",
    "            # Covered area represented in half-open intervals:\n",
    "            # [min_lat, max_lat)\n",
    "            # [min_lon, max_lon)\n",
    "            min_lat = math.floor(bounds[1])\n",
    "            min_lon = math.floor(bounds[0])\n",
    "            max_lat = math.floor(bounds[3]+1)\n",
    "            max_lon = math.floor(bounds[2]+1)\n",
    "            for lat in range(min_lat, max_lat):\n",
    "                for lon in range(min_lon, max_lon):\n",
    "                    idx.append_at_coords(lat, lon, relevant_files)\n",
    "                    \n",
    "            \n",
    "            if basename == \"EMSR333_02PORTOPALO_DEL_MONIT01_v1_observed_event_a\":\n",
    "                print(f\"Lat: [{min_lat}, {max_lat})\")\n",
    "                print(f\"Lon: [{min_lon}, {max_lon})\")\n",
    "                print(f\"Bounds: {bounds}\")\n",
    "                    \n",
    "print(f\"{count} valid records indexed out of {totalcount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "posted-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.save_index(\"/tank/ml4cc/nicholas/geographic_index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-helen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
